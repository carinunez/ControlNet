{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1-7YGdFb-GMYtRgg2Gsd4R-KL5LtGb-6G",
      "authorship_tag": "ABX9TyM6QkxcVt/HwcIaWOEVU5Wt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carinunez/ControlNet/blob/main/ControlNet_try2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ControlNet\n",
        "\n",
        "Integrantes:\n",
        "- xx\n",
        "- xx\n",
        "- xx\n",
        "-xx\n",
        "\n",
        "Arquitectura neuronal utilizada para agregar controles/restricciones especiales a los modelos de difusión de texto a imágenes preentrenados. En este caso, la arquitectura se aplico a Stable Diffusion, creando una copia entrenable de la UNet en el encoder que se une al modelo original utilizando zero-convolutions.\n",
        "\n",
        "\n",
        "Escoger qué condiciones se probarán-> pose, profundidad,\n"
      ],
      "metadata": {
        "id": "iEwWl75Jht-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import StableDiffusionPipeline, DDPMScheduler, UNet2DConditionModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n"
      ],
      "metadata": {
        "id": "JKG7nNYSekEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else None\n",
        "device"
      ],
      "metadata": {
        "id": "WB7ZHFWIlGYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id= \"runwayml/stable-diffusion-v1-5\""
      ],
      "metadata": {
        "id": "WLsoQdEgmQOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Carga de los datos\n"
      ],
      "metadata": {
        "id": "fUYJ1MMpxDai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ny8VQbZ098gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torchvision.transforms as T\n"
      ],
      "metadata": {
        "id": "lUETu3N94kP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Generación de Condiciones con Canny"
      ],
      "metadata": {
        "id": "FhNoFAYMNnkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = \"/content/drive/MyDrive/img_pkmn\"\n",
        "canny_dir  =\"/content/drive/MyDrive/canny\"\n",
        "\n",
        "def apply_canny_path(image: Image.Image, im_path, path=False) -> Image.Image:\n",
        "    if path:\n",
        "      img = np.array(image.convert(\"RGB\"))\n",
        "      edges = cv2.Canny(img, 100, 200)\n",
        "      edges = np.stack([edges] * 3, axis=-1)  # convertir a 3 canales\n",
        "      edges = Image.fromarray(edges)\n",
        "      edges.save(os.path.join(canny_dir, f'canny_{im_path}'))\n",
        "    else:\n",
        "      edges = Image.open(os.path.join(canny_dir, f'canny_{im_path}'))\n",
        "    return edges\n",
        "\n",
        "if not os.path.isdir(canny_dir):\n",
        "    os.makedirs(canny_dir)\n",
        "\n",
        "data = []\n",
        "for im_path in os.listdir(image_dir):\n",
        "  img = Image.open(os.path.join(image_dir, im_path)).convert(\"RGB\")\n",
        "  # si path es True, se generan las imag, sino se abren y cargan\n",
        "  canny = apply_canny_path(img, im_path, path=False)\n",
        "  prompt = \"a colorful pokemon\"\n",
        "  data.append({\"image\": img, 'canny':canny, \"text\": f\"a colorful pokemon_{im_path}\"})\n",
        "\n",
        "dataset = Dataset.from_list(examples)"
      ],
      "metadata": {
        "id": "mOFz8ABtNnGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "collapsed": true,
        "id": "utCqigCLapCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 12))\n",
        "ax1.imshow(data[0]['image']);\n",
        "ax1.set_title('Original');\n",
        "ax2.imshow(data[0]['canny']);\n",
        "ax2.set_title('Canny');\n"
      ],
      "metadata": {
        "id": "c2C6zS0dMux3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ControlDataset(Dataset):\n",
        "    def __init__(self, dataset, size=512):\n",
        "        self.dataset = dataset\n",
        "        self.image_trans = T.Compose([\n",
        "            T.Resize((size, size)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.5]*3, [0.5]*3)  # RGB\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      sample = self.dataset[idx]\n",
        "\n",
        "      img = self.image_trans(sample['image'])\n",
        "      ctrl = self.image_trans(sample['canny'])\n",
        "      prompt = sample[idx]\n",
        "      return img, ctrl, prompt\n"
      ],
      "metadata": {
        "id": "F4llFse_U1Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader\n",
        "dataset = Dataset.from_list(data)\n",
        "train_dataloader = DataLoader(ControlDataset(dataset), batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "j3pu4UEI4QdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se agrega un acelerador para hacer una prueba"
      ],
      "metadata": {
        "id": "QNQih3uk0Asz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DConditionModel, AutoencoderKL, DDIMScheduler\n",
        "from transformers import CLIPTextModel\n",
        "import torch\n",
        "\n",
        "# Carga el UNet preentrenado\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    model_id, subfolder=\"unet\"\n",
        ").to(device)\n",
        "\n",
        "# Carga el VAE preentrenado\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    model_id, subfolder=\"vae\"\n",
        ").to(device)\n",
        "\n",
        "# Carga el text encoder CLIP\n",
        "text_encoder = CLIPTextModel.from_pretrained(\n",
        "    \"openai/clip-vit-large-patch14\"\n",
        ").to(device)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = DDIMScheduler.from_pretrained(\n",
        "    model_id, subfolder=\"scheduler\"\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Py1FByEIk4Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Tokenizador\n",
        "\n",
        "Importamos el tokenizador de Stable Diffusion para evitar problemas/inconsistencias con el tamaño de los embeddings del texto c/r a la unet"
      ],
      "metadata": {
        "id": "MTMRjRd3paC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
        "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder='text_encoder')\n",
        "\n",
        "def get_text_embedding(prompt):\n",
        "  inputs = tokenizer(prompt, padding='max_length', trunctation=True,\n",
        "                     max_length=77, return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    return text_encoder(**inputs.to('cuda')).last_hidden_state"
      ],
      "metadata": {
        "id": "qeCOtpwFpZLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Bloques de ControlNet"
      ],
      "metadata": {
        "id": "0toEepGPx4un"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El bloque de ControlNet toma:\n",
        "- x: imagen ruidosa\n",
        "- t: paso de difusión\n",
        "- encoder_hidden_states: embeddings del texto\n",
        "- control_im: imagen utilizada para condicionar\n",
        "\n",
        "Se generan activaciones residuales de control_im con las zeroConv2d, luego inyecto estas activaciones a los bloques originales de Stable Diffusion\n",
        "Finalmente, se entrena manteniendo congelada la capa original de UNet"
      ],
      "metadata": {
        "id": "NL1Di1y0lpdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma de inyectar la condicion a StableDifussion\n",
        "class zeroConv2d(nn.Conv2d):\n",
        "    def __init__(self, in_c, out_c, kernel_size=1, stride=1, padding=0):\n",
        "        super().__init__(in_c, out_c, kernel_size, stride, padding)\n",
        "        nn.init.zeros_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)"
      ],
      "metadata": {
        "id": "qDCIPRC5hYyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MYControlNet(nn.Module):\n",
        "  def __init__(self, unet:UNet2DConditionModel):\n",
        "    super().__init__()\n",
        "    # .eval para congelar los pesos i.e no se entrenan\n",
        "    self.unet = unet.eval()\n",
        "\n",
        "    # copia de Stable Diffusion para entrenar\n",
        "    copy = unet.config\n",
        "    self.cond_proj = nn.ModuleList(\n",
        "        zeroConv2d(copy.in_channels, out_c, kernel_size=3, padding=1)\n",
        "        # aseguro que la copia tenga la misma cantidad de canales que cada\n",
        "        # cada bloque de DOWNSAMPLING\n",
        "        # Con esto puedo sumar directamente las activaciones a la salida en c/bloque\n",
        "        for out_c in copy.block_out_channels\n",
        "    )\n",
        "\n",
        "    # proyeccion bloque central/intermedio de la UNet -> BottleNeck\n",
        "    self.mid = zeroConv2d(copy.in_channels, copy.block_out_channels[-1], kernel_size=3, padding=1)\n",
        "\n",
        "  def forward(self, x, timesteps, encoder_hidden_states, control_im):\n",
        "    # Activaciones de control\n",
        "    cond_activ = [] # se guardan para usarlos como residual en los down_blocks\n",
        "\n",
        "    for proj in self.cond_proj:\n",
        "      cond = proj(control_im)\n",
        "      cond_activ.append(cond)\n",
        "\n",
        "    mid_activ = self.mid(control_im)\n",
        "\n",
        "    # se guardan los residuos para entregarselos al decoder mediante skip-connections\n",
        "    down_activ = []\n",
        "    h=x\n",
        "    for block, cond_res in zip(self.unet.down_blocks, cond_activ):\n",
        "      h = block(h, timesteps, encoder_hidden_states)\n",
        "      h = h + cond_res\n",
        "\n",
        "    h = self.unet.mid_block(h, timesteps, encoder_hidden_states)\n",
        "    h = h + mid_activ\n",
        "\n",
        "    for block, skip in zip(self.unet.up_blocks, reversed(down_activ)):\n",
        "      h = block(h, timesteps, encoder_hidden_states=skip)\n",
        "\n",
        "    return self.unet.out(h)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "od_DXSMgh42a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1 Entrenamiento pipeline"
      ],
      "metadata": {
        "id": "4vOog-t_sen-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "class MyPipeline(StableDiffusionPipeline):\n",
        "    def __init__(self, vae, text_encoder, tokenizer, my_controlnet):\n",
        "        super().__init__(vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=None)\n",
        "        self.my_controlnet = my_controlnet\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        # Aquí deberías adaptar el forward para llamar a tu my_controlnet.forward\n",
        "        pass\n",
        "\n",
        "    def __call__(self, prompt, control_image, **kwargs):\n",
        "        # Tokenizar texto\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=77)\n",
        "        encoder_hidden_states = self.text_encoder(**inputs).last_hidden_state\n",
        "\n",
        "        # Preparar latentes, timesteps, etc. (similar a pipeline normal)\n",
        "        latents = vae.encode(imgs).latent_dist.sample() * 0.18215\n",
        "        noise = torch.rand_like(img)\n",
        "        steps = torch.randoint(0, 1000, (img.shape[0],), device).long()\n",
        "        img_w_noise = noise_sch.add_noise(img, noise, steps)\n",
        "\n",
        "        # Usar MYControlNet para hacer la inferencia\n",
        "        output = self.my_controlnet(x=latents, timesteps=timesteps,\n",
        "                                    encoder_hidden_states=encoder_hidden_states,\n",
        "                                    control_im=control_image)\n",
        "\n",
        "        # Decodificar latentes con VAE, etc.\n",
        "        # ...\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "yc_0SbL2shsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Entrenamiento"
      ],
      "metadata": {
        "id": "E-lP6wutxtxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scheduler\n",
        "noise_sch = DDPMScheduler(num_train_timesteps=1000)"
      ],
      "metadata": {
        "id": "54nElH7rrDdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_CN(model, dataloader, optimizer, lr_scheduler,\n",
        "             epochs=10, lr=1e-4, save_each=3,\n",
        "             device='cuda'):\n",
        "  model.to('cuda')\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # model.train()\n",
        "    for i, (img, control_im, prompt) in enumerate(dataloader):\n",
        "      img = img.to(device)\n",
        "      ctrl_img = ctrl_img.to(device)\n",
        "      text_emb = get_text_embedding(prompt)\n",
        "\n",
        "      # inicializo el ruido\n",
        "      noise = torch.rand_like(img)\n",
        "      steps = torch.randoint(0, 1000, (img.shape[0],), device='cuda').long()\n",
        "      img_w_noise = noise_sch.add_noise(img, noise, steps)\n",
        "\n",
        "      # foward\n",
        "      pred = model(img_w_noise, steps, text_emb, control_im)\n",
        "\n",
        "      # loss\n",
        "      loss = F.mse_loss(pred, noise)\n",
        "\n",
        "      accelerator.backward(loss)\n",
        "      optimizer.step()\n",
        "      lr_scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      if (i+1) %10 == 0:\n",
        "        print(f'Epoca: {epoch}/epochs Step {steps} Loss: {loss.item():.4f}')\n",
        "\n",
        "      # Checkpoints\n",
        "      if epoch % save_each == 0:\n",
        "        accelerator.save_model(model, f\"./checkpoints/epoch_{epoch}\")\n"
      ],
      "metadata": {
        "id": "mWBe46J8rjsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Congelo la UNet\n",
        "model = MYControlNet(unet).to(device)\n",
        "for param in model.unet.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "gan_rFCJt5od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator(\n",
        "    mixed_precision=\"fp16\",  # o bf16\n",
        "    log_with=\"tensorboard\",  # o wandb\n",
        "    project_dir=\"./logs\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "                  filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                  lr=1e-4)\n",
        "\n",
        "# usado para un aprendizaje más suave\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=1000)\n",
        "\n",
        "model, optimizer, dataloader, lr_scheduler = accelerator.prepare(model, optimizer,\n",
        "                                                                 train_dataloader, lr_scheduler)\n"
      ],
      "metadata": {
        "id": "N-ZqxexmuT-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionControlNetPipeline\n",
        "from transformers import CLIPFeatureExtractor\n",
        "\n",
        "feature_extractor = CLIPFeatureExtractor.from_pretrained(model_id, subfolder=\"feature_extractor\")\n"
      ],
      "metadata": {
        "id": "H_lrfQ6JpKsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = StableDiffusionControlNetPipeline(\n",
        "    vae=vae,\n",
        "    text_encoder=text_encoder,\n",
        "    tokenizer=tokenizer,\n",
        "    unet=unet,\n",
        "    controlnet=ControlNet2,\n",
        "    safety_checker=None,\n",
        "    scheduler=scheduler,\n",
        "    feature_extractor=feature_extractor)"
      ],
      "metadata": {
        "id": "3Y3dxOpCpUGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2ND try"
      ],
      "metadata": {
        "id": "Gvg7c9svrUaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Asumiendo que tienes tu dataset con (input_latents, timesteps, encoder_hidden_states, control_images, target_latents)\n",
        "\n",
        "my_controlnet = MYControlNet(unet)\n",
        "\n",
        "optimizer = torch.optim.Adam(my_controlnet.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(10):\n",
        "    for batch in dataloader:\n",
        "        input_latents = batch[\"latents\"].to(device)\n",
        "        timesteps = batch[\"timesteps\"].to(device)\n",
        "        encoder_hidden_states = batch[\"encoder_hidden_states\"].to(device)\n",
        "        control_images = batch[\"control_images\"].to(device)\n",
        "        target = batch[\"target\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = my_controlnet(input_latents, timesteps, encoder_hidden_states, control_images)\n",
        "\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "-YLUxVRyrWKN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}