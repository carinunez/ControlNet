{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Gvg7c9svrUaV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carinunez/ControlNet/blob/main/ControlNet_try2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ControlNet\n",
        "\n",
        "Integrantes:\n",
        "- xx\n",
        "- xx\n",
        "- xx\n",
        "-xx\n",
        "\n",
        "Arquitectura neuronal utilizada para agregar controles/restricciones especiales a los modelos de difusión de texto a imágenes preentrenados. En este caso, la arquitectura se aplico a Stable Diffusion, creando una copia entrenable de la UNet en el encoder que se une al modelo original utilizando zero-convolutions.\n",
        "\n",
        "\n",
        "Escoger qué condiciones se probarán-> pose, profundidad,\n"
      ],
      "metadata": {
        "id": "iEwWl75Jht-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import StableDiffusionPipeline, DDPMScheduler, UNet2DConditionModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset, Dataset as HFDataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n"
      ],
      "metadata": {
        "id": "JKG7nNYSekEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else None\n",
        "device"
      ],
      "metadata": {
        "id": "WB7ZHFWIlGYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id= \"runwayml/stable-diffusion-v1-5\""
      ],
      "metadata": {
        "id": "WLsoQdEgmQOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Carga de los datos\n"
      ],
      "metadata": {
        "id": "fUYJ1MMpxDai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dFt7_UgeVJq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torchvision.transforms as T\n"
      ],
      "metadata": {
        "id": "lUETu3N94kP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Generación de Condiciones con Canny"
      ],
      "metadata": {
        "id": "FhNoFAYMNnkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = \"/content/drive/MyDrive/img_pkmn\"\n",
        "canny_dir  =\"/content/drive/MyDrive/canny\"\n",
        "\n",
        "if not os.path.isdir(canny_dir):\n",
        "    os.makedirs(canny_dir)\n",
        "\n",
        "def apply_canny_path(image: Image.Image, im_path) -> Image.Image:\n",
        "\n",
        "    img = np.array(image.convert(\"RGB\"))\n",
        "    edges = cv2.Canny(img, 100, 200)\n",
        "    edges = np.stack([edges] * 3, axis=-1)  # convertir a 3 canales\n",
        "    edges = Image.fromarray(edges)\n",
        "    edges.save(os.path.join(canny_dir, f'canny_{im_path}'))\n",
        "    return edges\n",
        "\n",
        "def load_images(folder, img_path):\n",
        "    return Image.open(os.path.join(folder, img_path)).convert('RGB')\n",
        "\n",
        "data = []\n",
        "for im_path in os.listdir(image_dir):\n",
        "  img = load_images(image_dir, im_path)\n",
        "  # canny = apply_canny_path(img, im_path)\n",
        "  canny = load_images(canny_dir, f'canny_{im_path}')\n",
        "  prompt = \"a colorful pokemon\"\n",
        "  data.append({\"image\": img, 'canny':canny, \"text\": f\"a colorful pokemon_{im_path}\"})\n",
        "\n",
        "dataset_hf = HFDataset.from_list(data)\n"
      ],
      "metadata": {
        "id": "mOFz8ABtNnGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 12))\n",
        "ax1.imshow(data[0]['image']);\n",
        "ax1.set_title('Original');\n",
        "ax2.imshow(data[0]['canny']);\n",
        "ax2.set_title('Canny');\n"
      ],
      "metadata": {
        "id": "c2C6zS0dMux3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ControlDataset(Dataset):\n",
        "    def __init__(self, dataset, size=512):\n",
        "        self.dataset = dataset\n",
        "        self.image_trans = T.Compose([\n",
        "            T.Resize((size, size)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.5]*3, [0.5]*3)  # RGB\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      sample = self.dataset[idx]\n",
        "\n",
        "      img = self.image_trans(sample['image'])\n",
        "      ctrl = self.image_trans(sample['canny'])\n",
        "      prompt = sample['text']\n",
        "      return img, ctrl, prompt\n"
      ],
      "metadata": {
        "id": "F4llFse_U1Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader\n",
        "# dataset = Dataset.from_list(data[:16])\n",
        "train_dataloader = DataLoader(ControlDataset(dataset_hf), batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "j3pu4UEI4QdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se agrega un acelerador para hacer una prueba"
      ],
      "metadata": {
        "id": "QNQih3uk0Asz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DConditionModel, AutoencoderKL, DDIMScheduler\n",
        "from transformers import CLIPTextModel\n",
        "import torch\n",
        "\n",
        "# Carga el UNet preentrenado\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    model_id, subfolder=\"unet\"\n",
        ").to(device)\n",
        "\n",
        "# Carga el VAE preentrenado\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    model_id, subfolder=\"vae\"\n",
        ").to(device)\n",
        "\n",
        "# Carga el text encoder CLIP\n",
        "text_encoder = CLIPTextModel.from_pretrained(\n",
        "    \"openai/clip-vit-large-patch14\"\n",
        ").to(device)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = DDIMScheduler.from_pretrained(\n",
        "    model_id, subfolder=\"scheduler\"\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Py1FByEIk4Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Tokenizador\n",
        "\n",
        "Importamos el tokenizador de Stable Diffusion para evitar problemas/inconsistencias con el tamaño de los embeddings del texto c/r a la unet"
      ],
      "metadata": {
        "id": "MTMRjRd3paC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
        "text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder='text_encoder')\n",
        "\n",
        "def get_text_embedding(prompt):\n",
        "  inputs = tokenizer(prompt, padding='max_length', trunctation=True,\n",
        "                     max_length=77, return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    return text_encoder(**inputs.to('cuda')).last_hidden_state"
      ],
      "metadata": {
        "id": "qeCOtpwFpZLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Bloques de ControlNet"
      ],
      "metadata": {
        "id": "0toEepGPx4un"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El bloque de ControlNet toma:\n",
        "- x: imagen ruidosa\n",
        "- t: paso de difusión\n",
        "- encoder_hidden_states: embeddings del texto\n",
        "- control_im: imagen utilizada para condicionar\n",
        "\n",
        "Se generan activaciones residuales de control_im con las zeroConv2d, luego inyecto estas activaciones a los bloques originales de Stable Diffusion\n",
        "Finalmente, se entrena manteniendo congelada la capa original de UNet"
      ],
      "metadata": {
        "id": "NL1Di1y0lpdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Congelar los pesos de la UNet, VAE y Text Encoder\n",
        "unet.requires_grad_(False)\n",
        "vae.requires_grad_(False)\n",
        "text_encoder.requires_grad_(False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JoSDlmpiyoRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scheduler\n",
        "# noise_sch = DDPMScheduler(num_train_timesteps=1000)\n",
        "noise_sch  =scheduler"
      ],
      "metadata": {
        "id": "54nElH7rrDdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma de inyectar la condicion a StableDifussion\n",
        "class zeroConv2d(nn.Conv2d):\n",
        "    def __init__(self, in_c, out_c, kernel_size=1, stride=1, padding=0):\n",
        "        super().__init__(in_c, out_c, kernel_size, stride, padding)\n",
        "        nn.init.zeros_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)"
      ],
      "metadata": {
        "id": "qDCIPRC5hYyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1 Entrenamiento pipeline"
      ],
      "metadata": {
        "id": "4vOog-t_sen-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma de inyectar la condicion a StableDifussion\n",
        "class zeroConv2d(nn.Conv2d):\n",
        "    def __init__(self, in_c, out_c, kernel_size=1, stride=1, padding=0):\n",
        "        super().__init__(in_c, out_c, kernel_size, stride, padding)\n",
        "        nn.init.zeros_(self.weight)\n",
        "        if self.bias is not None:\n",
        "            nn.init.zeros_(self.bias)\n",
        "\n",
        "class MYControlNet(nn.Module):\n",
        "    def __init__(self, unet:UNet2DConditionModel, condition_image_channels=3):\n",
        "        super().__init__()\n",
        "        # ControlNet no necesita la UNet internamente para su forward pass principal\n",
        "        # Solo necesita su configuración para construir sus propias capas.\n",
        "        unet_config = unet.config\n",
        "        block_channels = unet_config.block_out_channels\n",
        "\n",
        "        # Bloques de ControlNet que reflejan los down_blocks de la UNet\n",
        "        self.conv_in = zeroConv2d(condition_image_channels,\n",
        "                                  unet_config.block_out_channels[0],\n",
        "                                  kernel_size=3, padding=1)\n",
        "\n",
        "        self.input_blocks = nn.ModuleList()\n",
        "        in_ch = block_channels[0]\n",
        "\n",
        "        for out_ch in unet_config.block_out_channels[1:]:\n",
        "            self.input_blocks.append(\n",
        "                nn.Sequential(\n",
        "                    zeroConv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "                    nn.AvgPool2d(2)  # Reduce resolución a la mitad\n",
        "                )\n",
        "            )\n",
        "            in_ch = out_ch\n",
        "\n",
        "        # Bloque central/intermedio de ControlNet\n",
        "        self.middle_block = nn.Sequential(\n",
        "                                zeroConv2d(in_ch, in_ch, kernel_size=3, padding=1),\n",
        "                                nn.SiLU(),\n",
        "                                zeroConv2d(in_ch, in_ch, kernel_size=3, padding=1))\n",
        "\n",
        "\n",
        "    def forward(self, sample, timestep=None, encoder_hidden_states=None,\n",
        "                controlnet_cond=None, **kwargs):\n",
        "       # ControlNet solo toma la imagen de control\n",
        "        # Generar las activaciones residuales de ControlNet\n",
        "        controlnet_down_res = []\n",
        "\n",
        "        # Primera activación (después de `conv_in`)\n",
        "        h = self.conv_in(controlnet_cond)\n",
        "        controlnet_down_res.append(h)\n",
        "\n",
        "        # Iterar a través de los bloques de entrada de ControlNet\n",
        "        for block in self.input_blocks:\n",
        "            h = block(h)\n",
        "            controlnet_down_res.append(h)\n",
        "\n",
        "        # Activación del bloque central\n",
        "        controlnet_mid_res = self.middle_block(h) # La última 'h' es la entrada al mid_block\n",
        "\n",
        "        return controlnet_down_res, controlnet_mid_res"
      ],
      "metadata": {
        "id": "tpylGMst0ZB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Congelo la UNet\n",
        "model = MYControlNet(unet).to('cuda')\n",
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n"
      ],
      "metadata": {
        "id": "gan_rFCJt5od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SUVTth2k1OWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet.to(device)\n",
        "unet.eval()\n",
        "\n",
        "vae.to(device)\n",
        "vae.eval()\n",
        "\n",
        "text_encoder.to(device)\n",
        "text_encoder.eval()\n",
        "for param in text_encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "optimizer = torch.optim.AdamW(list(model.parameters()) + list(unet.parameters()),\n",
        "                              lr=1e-4)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch_idx, (img_batch, contrl_batch, prompt_batch) in enumerate(tqdm(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mover datos a la GPU\n",
        "        target_latents = img_batch.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          latent_dist = vae.encode(target_latents).latent_dist\n",
        "          target_latents = latent_dist.sample()*vae.config.scaling_factor\n",
        "\n",
        "        control_images = contrl_batch.to(device)\n",
        "        control_images = torch.nn.functional.interpolate(control_images,\n",
        "                                                         size=(512,512),\n",
        "                                                         mode=\"bilinear\",\n",
        "                                                         align_corners=False)\n",
        "        text_prompts = prompt_batch\n",
        "\n",
        "        # 1. Codificar el texto de condición\n",
        "        text_inputs = tokenizer(\n",
        "                          text_prompts,\n",
        "                          padding=\"max_length\",\n",
        "                          truncation=True,\n",
        "                          max_length=tokenizer.model_max_length,\n",
        "                          return_tensors=\"pt\",\n",
        "                          ).input_ids.to(device)\n",
        "        encoder_hidden_states = text_encoder(text_inputs)[0]\n",
        "\n",
        "        # 2. Generar un timestep aleatorio\n",
        "        timesteps = torch.randint(0, noise_sch.num_train_timesteps, (target_latents.shape[0],), device=device).long()\n",
        "\n",
        "        # 3. Añadir ruido a los latents objetivos\n",
        "        noise = torch.randn_like(target_latents)\n",
        "        noisy_latents = noise_sch.add_noise(target_latents, noise, timesteps)\n",
        "\n",
        "        # Las activaciones de ControlNet:\n",
        "        controlnet_down_res, controlnet_mid_res = model(\n",
        "                                sample=noisy_latents,  # ignored inside MYControlNet\n",
        "                                timestep=timesteps,    # ignored inside MYControlNet\n",
        "                                encoder_hidden_states=encoder_hidden_states,  # ignored inside MYControlNet\n",
        "                                controlnet_cond=control_images)\n",
        "\n",
        "        # controlnet_down_block_res_samples =controlnet_outputs.down_block_res_samples\n",
        "        # controlnet_mid_block_res_sample = controlnet_outputs.mid_block_res_samples\n",
        "        # controlnet_down_block_res_samples= list(reversed(controlnet_down_block_res_samples))\n",
        "\n",
        "        for i, r in enumerate(controlnet_down_block_res_samples):\n",
        "            print(f\"  Residual {i}: {r.shape}\")\n",
        "        print(f\"  Mid residual: {controlnet_mid_block_res_sample.shape}\")\n",
        "\n",
        "\n",
        "        model_pred = unet(\n",
        "            noisy_latents,\n",
        "            timesteps,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            down_block_additional_residuals=controlnet_down_res, # Lista de tensores\n",
        "            mid_block_additional_residual=controlnet_mid_res,     # Tensor\n",
        "            return_dict=False)[0] # Obtener el tensor de salida, no el dict\n",
        "\n",
        "        # 6. Calcular la pérdida\n",
        "        # Se compara la predicción del ruido con el ruido real.\n",
        "        loss = torch.nn.functional.mse_loss(model_pred, noise)\n",
        "\n",
        "        # 7. Retropropagación\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (batch_idx + 1) % 10 == 0:\n",
        "      print(f\"Step {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "        # Checkpoints\n",
        "        # --- Save model every 10 epochs ---\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "      checkpoint_path = f\"./checkpoints/epoch_{epoch+1}.pth\"\n",
        "      torch.save(model.state_dict(), checkpoint_path)\n",
        "      print(f\"Model saved to {checkpoint_path}\")\n",
        "    print(\"Entrenamiento completado.\")\n",
        "\n",
        "# Guardar tu ControlNet\n",
        "torch.save(model.state_dict(), \"my_controlnet_weights.pth\")"
      ],
      "metadata": {
        "id": "pZvQPfLh00Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Entrenamiento"
      ],
      "metadata": {
        "id": "E-lP6wutxtxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator(\n",
        "    mixed_precision=\"fp16\",  # o bf16\n",
        "    log_with=\"tensorboard\",  # o wandb\n",
        "    project_dir=\"./logs\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "                  filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                  lr=1e-4)\n",
        "\n",
        "# usado para un aprendizaje más suave\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=1000)\n",
        "\n",
        "model, optimizer, dataloader, lr_scheduler = accelerator.prepare(model, optimizer,\n",
        "                                                                 train_dataloader, lr_scheduler)\n"
      ],
      "metadata": {
        "id": "N-ZqxexmuT-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}